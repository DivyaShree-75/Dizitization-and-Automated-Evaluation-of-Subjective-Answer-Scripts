# -*- coding: utf-8 -*-
"""Trainingmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dXPNIJB937pMPU2ltcKVE8ryylR-C06
"""

import torch
import torch.nn as nn
import pandas as pd
import json
from sentence_transformers import SentenceTransformer, util
import nltk
from nltk.corpus import wordnet
import re
import os

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('popular')

# Load SBERT model
base_model = SentenceTransformer("all-MiniLM-L6-v2")
original_transformer = base_model._first_module().auto_model

class CustomSBERT(nn.Module):
    def _init_(self, original_model):
        super()._init_()
        self.original_model = original_model
        self.extra_layer = nn.TransformerEncoderLayer(d_model=384, nhead=6)

    def forward(self, input_ids, attention_mask=None):
        outputs = self.original_model(input_ids, attention_mask=attention_mask)
        return self.extra_layer(outputs.last_hidden_state)

def load_or_initialize_custom_model(model_path):
    model = CustomSBERT(original_transformer)
    if not os.path.exists(model_path):
        torch.save({"model_state_dict": model.state_dict()}, model_path)
        print("âœ… Custom SBERT model saved!")
    else:
        state_dict = torch.load(model_path, map_location=torch.device("cpu"))
        model.load_state_dict(state_dict["model_state_dict"], strict=False)
        model.eval()
        print("âœ… Custom SBERT model loaded!")
    return model

def get_synonyms(word):
    return list({lemma.name().replace("_", " ").lower()
                 for syn in wordnet.synsets(word)
                 for lemma in syn.lemmas()})

def simple_sentence_tokenize(text):
    return [s.strip() for s in re.split(r'[.!?]+\s+', text) if s.strip()]

def check_keyword_match(answer, keyword):
    answer = answer.lower()
    keyword = keyword.lower()

    if keyword in answer:
        return 1.0

    for synonym in get_synonyms(keyword):
        if synonym in answer:
            return 0.8

    try:
        sim = float(util.cos_sim(base_model.encode(keyword, convert_to_tensor=True),
                                 base_model.encode(answer, convert_to_tensor=True))[0][0])
        return 0.6 if sim > 0.7 else 0.0
    except:
        return 0.0

def enhanced_keyword_relevance(answer, keywords):
    if not keywords or not answer.strip():
        return 0.0
    try:
        sentences = nltk.sent_tokenize(answer)
    except:
        sentences = simple_sentence_tokenize(answer)

    scores = []
    for kw in keywords:
        max_score = max(check_keyword_match(sent, kw) for sent in sentences)
        scores.append(max_score)
    return sum(scores) / len(scores) if scores else 0.0

def compute_semantic_similarity(ans, model_ans):
    if not ans.strip():
        return 0.0
    emb1 = base_model.encode(ans, convert_to_tensor=True)
    emb2 = base_model.encode(model_ans, convert_to_tensor=True)
    return float(util.cos_sim(emb1, emb2)[0][0])

def evaluate_excel_with_model_answers(cleaned_excel_path, model_answer_path, output_path, model_dir):
    df = pd.read_excel(cleaned_excel_path)
    with open(model_answer_path, "r", encoding="utf-8") as f:
        model_answers = json.load(f)

    model_answers_dict = {str(item["question_number"]): item["answer"] for item in model_answers}
    keywords_dict = {str(item["question_number"]): item.get("keywords", []) for item in model_answers}

    model = load_or_initialize_custom_model(model_dir)

    semantic_scores, keyword_scores, final_scores = [], [], []

    for _, row in df.iterrows():
        q_no = str(row["Question Number"]).strip()
        student_ans = str(row["Answer"]).strip() if pd.notna(row["Answer"]) else ""

        if q_no in model_answers_dict:
            model_ans = model_answers_dict[q_no]
            keywords = keywords_dict.get(q_no, [])

            sem_score = compute_semantic_similarity(student_ans, model_ans)
            key_score = enhanced_keyword_relevance(student_ans, keywords)
            final_score = round(min(1.0, 0.8 * sem_score + 0.2 * key_score), 4)
        else:
            sem_score, key_score, final_score = None, None, None

        semantic_scores.append(sem_score)
        keyword_scores.append(key_score)
        final_scores.append(final_score)

    df["Semantic Similarity"] = semantic_scores
    df["Keyword Relevance"] = keyword_scores
    df["Final Similarity Score"] = final_scores

    df.drop(columns=["Keywords Matched"], errors="ignore", inplace=True)
    df.to_excel(output_path, index=False)
    print(f"\nðŸ“Š Saved enhanced scores to: {output_path}")
    print(df[["Question Number", "Keyword Relevance", "Final Similarity Score"]].head())