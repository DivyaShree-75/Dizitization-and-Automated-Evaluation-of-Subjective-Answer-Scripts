# -*- coding: utf-8 -*-
"""ocr_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y85Jpc3UScez6h7doVRZMkgKUUgKByah
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install pdf2image
!apt-get install poppler-utils

import os
import cv2
import numpy as np
from pdf2image import convert_from_path
from PIL import Image
from google.colab import drive



pdf_folder = "/content/drive/MyDrive/SIP_PDF"
output_folder = "/content/drive/MyDrive/SIP_PDF_TO_IMAGE"

os.makedirs(output_folder, exist_ok=True)


pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(".pdf")]

t
if not pdf_files:
    print("❌ No PDFs found in the folder. Please check the folder path.")
    exit()


def add_large_page_number_cv(image_pil, page_no):
    image_cv = np.array(image_pil)
    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)

    text = f"Page {page_no}"
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 5
    font_thickness = 12
    text_color = (0, 0, 0)

    (text_width, _), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
    text_x = (image_cv.shape[1] - text_width) // 2
    text_y = 100

    cv2.putText(image_cv, text, (text_x, text_y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    return Image.fromarray(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))


def preprocess_image_cv(image_pil):
    img = np.array(image_pil.convert('L'))  # Convert to grayscale
    img = cv2.GaussianBlur(img, (5, 5), 0)
    img = cv2.adaptiveThreshold(img, 255,
                                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 11, 2)
    return Image.fromarray(img)


for pdf_file in pdf_files:
    pdf_path = os.path.join(pdf_folder, pdf_file)
    pdf_name = os.path.splitext(pdf_file)[0]


    images_folder = os.path.join(output_folder, pdf_name)
    os.makedirs(images_folder, exist_ok=True)


    images = convert_from_path(pdf_path, dpi=300)

    for i, image in enumerate(images):
        image_with_page = add_large_page_number_cv(image, i + 1)

        preprocessed_image = preprocess_image_cv(image_with_page)

        image_path = os.path.join(images_folder, f"page_{i+1}.png")

        try:
            preprocessed_image.save(image_path, "PNG")
        except Exception as e:
            print(f" Error saving image {image_path}: {e}")

    print(f" Processed '{pdf_file}' with page numbers and OCR preprocessing.")

print("All PDFs processed with page numbers and preprocessed for OCR!")

from google.colab import files

# Upload the JSON key manually
uploaded = files.upload()

# Move the uploaded JSON file to a known location
!mkdir -p ~/.config/gcloud
!mv cloud-vision.json ~/.config/gcloud/application_default_credentials.json

import os

# Set GOOGLE_APPLICATION_CREDENTIALS environment variable
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/root/.config/gcloud/application_default_credentials.json"

# Authenticate using the service account
!gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS

# Install Tesseract OCR and pytesseract
!sudo apt install tesseract-ocr -y
!pip install pytesseract

!pip install google-cloud-vision

import os
import json
import re
import io
from PIL import Image
from google.cloud import vision

vision_client = vision.ImageAnnotatorClient()

image_root = "/content/drive/MyDrive/SIP_PDF_TO_IMAGE"
output_json_folder = "/content/drive/MyDrive/SIP_JSON_OUTPUT5"
os.makedirs(output_json_folder, exist_ok=True)

student_folders = [folder for folder in os.listdir(image_root) if os.path.isdir(os.path.join(image_root, folder))]

if not student_folders:
    print("❌ No student folders found. Please check the input path.")
    exit()


for student_id in student_folders:
    student_folder = os.path.join(image_root, student_id)
    page_images = sorted([f for f in os.listdir(student_folder) if f.endswith(".png")])

    json_data = {
        "student_id": student_id,
        "pages": []
    }

    for image_name in page_images:
        try:
            page_number = int(image_name.split("_")[1].split(".")[0])
            image_path = os.path.join(student_folder, image_name)

            # Read image bytes
            with io.open(image_path, 'rb') as image_file:
                content = image_file.read()

            image = vision.Image(content=content)


            response = vision_client.text_detection(image=image)
            annotations = response.text_annotations

            if annotations:
                text = annotations[0].description
            else:
                text = ""

            text_lines = text.splitlines()
            cleaned_lines = []
            for line in text_lines:

                if re.match(r'^\s*page\s*\d+\s*$', line, re.IGNORECASE):
                    continue
                if not re.match(r'^\s*rage\s*[\|&\d]*\s*$', line, re.IGNORECASE):
                    cleaned_lines.append(line.strip())


            cleaned_text = ' '.join(cleaned_lines)
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()


            json_data["pages"].append({
                "page_number": page_number,
                "text": cleaned_text
            })

        except Exception as e:
            print(f"❌ Error processing {image_name} for {student_id}: {e}")


    output_path = os.path.join(output_json_folder, f"{student_id}.json")
    try:
        with open(output_path, "w") as f:
            json.dump(json_data, f, indent=4)
        print(f" Extracted text saved for {student_id}")
    except Exception as e:
        print(f"❌ Error writing JSON for {student_id}: {e}")

print("All student OCR extractions completed and saved as JSON.")